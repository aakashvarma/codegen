model_config:
  model_name: "codellama/CodeLlama-7b-hf"
  cache_dir: null
  bits: 4
  double_quant: true
  quant_type: "nf4"
  trust_remote_code: false
  use_auth_token: false
  compute_type: "fp16"

finetune_config:
  r: 16
  lora_alpha: 32
  lora_dropout: 0.05

trainer_config:
  dataset_name: "b-mc2/sql-create-context"
  block_size: 512
  model_output_dir: "__run.default"
  per_device_train_batch_size: 4
  gradient_accumulation_steps: 4
  optim: "paged_adamw_32bit"
  save_steps: 100
  logging_steps: 10
  learning_rate: 0.0002
  max_grad_norm: 0.3
  max_steps: 100
  warmup_ratio: 0.03
  lr_scheduler_type: "constant"
  compute_type: "fp16"